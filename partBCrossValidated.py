'''
This code uses neural networks to model a dataset generated by a second degree polynomial.
It tests 3 different activation functions and compares the results with a benchmark of regular OLS regression.
It uses crossvalidation with 5 folds.
'''

from neural_network import FFNN
from cost_functions import *
from schedulers import *
from activation_functions import *
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import time

import seaborn as sns

sns.set_theme()

#Define path for saving figures.
import os
from pathlib import Path
cwd = os.getcwd()
path = Path(cwd) / "FigurePlots" / "B_NeuralNetwork"

if not path.exists():
    path.mkdir()

def plot_etas(etas, MSEs, title, filename):
    '''
    Description:
    ------------
    Function for plotting MSE by eta values.
    Parameters:
    ------------
        I   etas list(floats): eta values tested
        II  MSEs list(floats): MSE for the eta values
        III title str: Title for plot
        IV filename str | Path: path for saving figure

    Returns:
    ------------
        I   None
    '''
    plt.figure()
    plt.plot(etas, MSEs)
    plt.xscale("log")
    plt.title("Validation MSE for method" + title)
    plt.tight_layout()
    plt.savefig(path / filename)



#Define data
def f(x):
    return -3+5 * x -3*x**2

np.random.seed(13)
n_datapoints = 1000
x = np.random.rand(n_datapoints, 1)
y = f(x) + 0.3*np.random.randn(n_datapoints, 1)

#Make train and test set split indices for Crossvalidation
from sklearn.model_selection import KFold

n_folds = 5
kf = KFold(n_splits=n_folds)

#print(f"Train test splits: {indicesX}, {indicesY}")
#Define hyperparameters
n_etas = 10
etas = np.geomspace(10**(-7), 1, n_etas)
n_lams = 10
lams = np.geomspace(10**(-7), 1, n_lams)
batches = 10
epochs = 20
seed = 13

# Define topology of neural network. 1 Hidden layer with 10 neurons.
dimensions = (1, 20, 1)

#Keep track of best MSE by acctivation method
MSE_method = []
method_names =  [] 


#Neural network with activation function sigmoid.
method_name= "Sigmoid"
MSEs = np.zeros((n_folds, n_etas))
t0 = time.time()
for i, (train_index, test_index) in  enumerate(kf.split(x, y)):
    for j, eta in enumerate(etas):
        NN = FFNN(dimensions, hidden_func=sigmoid, output_func=identity, cost_func=CostOLS, seed=seed)
        scores = NN.fit(x[train_index], y[train_index], RMS_prop(eta, 0.9), batches=batches, epochs=epochs, X_val = x[test_index], t_val = y[test_index])
        MSEs[i][j] = np.min(scores["val_errors"])
t1 = time.time()
total = t1-t0

print(f"Time for Sigmoid = {total}")

plot_etas(etas, np.mean(MSEs, axis=0), method_name, method_name + ".png")
print(f'Best over folds: {np.min(MSEs, axis=1)}')
print(np.mean(np.min(MSEs, axis=1)))
MSE_method.append(np.mean(np.min(MSEs, axis=1)))
method_names.append(method_name)
print(f"\n Best MSE train sigmoid is {np.min(MSEs)}")

#Neural network with activation function RELU
method_name= "RELU"
MSEs = np.zeros((n_folds, n_etas))
t0 = time.time()
for i, (train_index, test_index) in  enumerate(kf.split(x, y)):
    for j, eta in enumerate(etas):
        NN = FFNN(dimensions, hidden_func=RELU, output_func=identity, cost_func=CostOLS, seed=seed)
        scores = NN.fit(x[train_index], y[train_index], RMS_prop(eta, 0.9), batches=batches, epochs=epochs, X_val = x[test_index], t_val = y[test_index])
        MSEs[i][j] = np.min(scores["val_errors"])
t1 = time.time()
total = t1-t0

print(f"Time for RELU = {total}")

plot_etas(etas, np.mean(MSEs, axis=0), method_name, method_name + ".png")
print(f'Best over folds: {np.min(MSEs, axis=1)}')
print(np.mean(np.min(MSEs, axis=1)))
MSE_method.append(np.mean(np.min(MSEs, axis=1)))
method_names.append(method_name)
print(f"\n Best MSE train RELU is {np.min(MSEs)}")


#Neural network with activation function Leaky RELU
method_name= "Leaky_RELU"
MSEs = np.zeros((n_folds, n_etas))
t0 = time.time()
for i, (train_index, test_index) in  enumerate(kf.split(x, y)):
    print(f"Fold {i} \n")
    for j, eta in enumerate(etas):
        NN = FFNN(dimensions, hidden_func=LRELU, output_func=identity, cost_func=CostOLS, seed=seed)
        scores = NN.fit(x[train_index], y[train_index], RMS_prop(eta, 0.9), batches=batches, epochs=epochs, X_val = x[test_index], t_val = y[test_index])
        MSEs[i][j] = np.min(scores["val_errors"])
t1 = time.time()
total = t1-t0

print(f"Time for Leaky RELU = {total}")

plot_etas(etas, np.mean(MSEs, axis=0), method_name, method_name + ".png")
print(f'Best over folds: {np.min(MSEs, axis=1)}')
print(np.mean(np.min(MSEs, axis=1)))
MSE_method.append(np.mean(np.min(MSEs, axis=1)))
method_names.append(method_name)
print(f"\n Best MSE train LRELU is {np.min(MSEs)}")


#Do a regular OLS for benchmarking
method_name = "OLS_regression"
from OLS_regression import ols_regression
MSEs = np.zeros(n_folds)
t0 = time.time()
for i, (train_index, test_index) in  enumerate(kf.split(x, y)):
    X_train = np.c_[np.ones(x[train_index].size), x[train_index], x[train_index]**2]
    X_test = np.c_[np.ones(x[test_index].size), x[test_index], x[test_index]**2]
    pred_train, pred_val, thetas = ols_regression(X_train, X_test, y[train_index])

    MSEs[i] = CostOLS(y[test_index])(pred_val)
t1 = time.time()
total = t1-t0

print(f"Time for Ols regression = {total}")
MSE_method.append(np.mean(MSEs))
method_names.append(method_name)


print(f"Avg MSE {MSE_method}")

#Define new folder for saving figure
path = Path(cwd) / "FigurePlots" / "Neural_Network_By_method"
if not path.exists():
    path.mkdir()


#Plot bar chart of best MSE by activation function
plt.figure()
plt.bar(range(0, 2*len(method_names), 2), MSE_method,  tick_label =method_names)
plt.title("Neural network best MSE val. error by hidden activation")
plt.xticks(rotation=-30)
plt.tight_layout()
plt.savefig(path / "activation_func.png")


